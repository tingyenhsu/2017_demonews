{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Facebook\n",
    "* ChinaTimes 188311137478\n",
    "* udn 241284961029\n",
    "* ltn 394896373929368\n",
    "* AppleDaily 232633627068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import requests.packages.urllib3\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import urllib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fb_token(app_id, app_secret):           \n",
    "    payload = {'grant_type': 'client_credentials', \n",
    "               'client_id': app_id, \n",
    "               'client_secret': app_secret}\n",
    "    file = requests.post('https://graph.facebook.com/oauth/access_token?', \n",
    "                         params = payload)\n",
    "    result = file.text.split(\"=\")[1]\n",
    "    return result\n",
    "\n",
    "def query_url(pageid, q, token):\n",
    "    return \"https://graph.facebook.com/v2.6/%s?%s&access_token=%s\"%(pageid, q, token)\n",
    "#     \n",
    "pageid = '188311137478' #ChinaTimes \n",
    "# pageid = '241284961029' #udn \n",
    "# pageid = '394896373929368' #ltn \n",
    "# pageid = '232633627068' #AppleDaily \n",
    "app_id = '1577374979150890'\n",
    "app_secret = '6408b3bcaabd6a87d65200976f76a387'\n",
    "\n",
    "token = 'EAACEdEose0cBABX10l1ZC7RIhSY1NGIrbHKL3oCQAdkZCJGbBl5YDlBZB4es4e9GOhJqXZCzXZBPHSXqw0a3XSkQZBaMtpOOItGF2H5xEzfEIa7FS0BhAiJImIcH6a2nLgtk8vZBf7sPzb6ZBs7LLcfpypp4bsM3ZBUm8bbWd6mdhT0Te1TmAk4v9'\n",
    "token = get_fb_token(app_id, app_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Getting post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'created_time': u'2017-01-16T04:39:00+0000', u'message': u'\\u9084\\u5305\\u819c\\u5594 ^_<  #\\u5c3e\\u7259', u'permalink_url': u'https://www.facebook.com/myudn/posts/10155366213971030', u'id': u'241284961029_10155366213971030'}\n"
     ]
    }
   ],
   "source": [
    "directory = \"data/%s/\"%pageid\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "directory = directory + datetime.datetime.now().strftime('%y%m%d%H%M%S') + '/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "\n",
    "url = query_url(pageid, 'fields=posts{shares,message,permalink_url,created_time}', token)\n",
    "results = requests.get(url)\n",
    "data = json.loads(results.text)\n",
    "print data['posts']['data'][0]\n",
    "# # posts = data['posts']['data']\n",
    "# # print data['posts']['paging']['next']\n",
    "# # url = data['posts']['paging']['next']\n",
    "\n",
    "# # pids = [p['id'] for p in data['posts']['data']]\n",
    "# while True:\n",
    "#     results = requests.get(url)\n",
    "#     data = json.loads(results.text)\n",
    "#     if len(data['data']) == 0:\n",
    "#         break    \n",
    "#     posts += data['data']\n",
    "# #     print data['paging']['next']\n",
    "#     url = data['paging']['next']\n",
    "#     print 'Total post:%d'%len(posts)\n",
    "\n",
    "# # print len(posts)\n",
    "# fname = directory + 'posts.json'\n",
    "# json.dump(posts, open(fname, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Get comments of each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCofC(comments, fname, onlyComment=True): # getting comment of comment\n",
    "    allC = []\n",
    "    for i, c in enumerate(comments):\n",
    "        if onlyComment:\n",
    "            if c['comment_count'] != 0:\n",
    "                print \"C%d[%s]:\\tCC:%d\\t\"%(i, c['id'],c['comment_count'])\n",
    "                url = query_url(c['id'], 'fields=comments.limit(%d)'%c['comment_count'], token)\n",
    "\n",
    "                results = requests.get(url)\n",
    "                data = json.loads(results.text)\n",
    "                allC.append({c['id']:{'cc':[] if 'comments' not in data else data['comments']['data'], \n",
    "                                      'like':[] if 'likes' not in data else data['likes']['data']\n",
    "                                     }})\n",
    "        else:\n",
    "            if c['comment_count'] != 0 or c['like_count'] != 0:\n",
    "                print \"C%d[%s]:\\tCC:%d\\tLike:%d\"%(i, c['id'],c['comment_count'],c['like_count'])\n",
    "                url = query_url(c['id'], 'fields=comments.limit(%d),likes.limit(%d)'%(c['comment_count'], c['like_count']), token)\n",
    "\n",
    "                results = requests.get(url)\n",
    "                data = json.loads(results.text)\n",
    "                allC.append({c['id']:{'cc':[] if 'comments' not in data else data['comments']['data'], \n",
    "                                      'like':[] if 'likes' not in data else data['likes']['data']\n",
    "                                     }})\n",
    "    with open(fname+'.test', 'w') as fout:\n",
    "        json.dump(allC, fout)\n",
    "    print \"...Succesfully Save\", fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Get comments of each posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "DEBUG = False\n",
    "amount = 50 if DEBUG else 500\n",
    "print \"Length of posts: %d\"%len(posts)\n",
    "\n",
    "for i, p in enumerate(posts):\n",
    "#     if i < 218:\n",
    "#         continue\n",
    "#     if i not in [23]:\n",
    "#         continue\n",
    "    \n",
    "#     url = query_url(p['id'], 'fields=comments.limit(500){like_count,comment_count,created_time,from,message}', token)\n",
    "    url = query_url(p['id'], 'fields=comments.limit(%d){comment_count,like_count,created_time,from,message}'%amount, token)\n",
    "    results = requests.get(url)\n",
    "    data = json.loads(results.text)\n",
    "    if len(data.keys()) == 1:\n",
    "        print \"ERROR:CANNOT GET THE POST:\", data\n",
    "        continue\n",
    "    data = data['comments']\n",
    "    comments = data['data']\n",
    "    if 'next' in data['paging']:\n",
    "        while True:\n",
    "            url = data['paging']['next']\n",
    "            results = requests.get(url)\n",
    "            data = json.loads(results.text)\n",
    "            try:\n",
    "                comments += data['data']\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                print \"ERROR: time sleep 3 seconds\"\n",
    "                print data.keys()\n",
    "                comments  += data['data']\n",
    "            if 'next' not in data['paging']:\n",
    "                break\n",
    "    print 'PostID[%d] %s (comments:%d) time:%d'%(i, p['id'], len(comments), time.time()-start)\n",
    "    print p['permalink_url']\n",
    "    \n",
    "    getCofC(comments, directory + 'c2cment%d-%s.json'%(i, p['id']))\n",
    "    fname = directory + 'comment%d-%s.json'%(i, p['id'])\n",
    "    with open(fname, 'w') as fout:\n",
    "        json.dump(comments, fout)\n",
    "#####################################################################################\n",
    "# Getting comments of comments\n",
    "    maxcc = max(c['comment_count'] for c in comments)\n",
    "    if DEBUG: print('maxcc:%d'%maxcc)\n",
    "    url = query_url(p['id'], 'fields=comments.limit(%d){comments.limit(%d)}'%(amount, maxcc), token)\n",
    "    results = requests.get(url)\n",
    "    data = json.loads(results.text)\n",
    "    if len(data.keys()) == 1:\n",
    "        print \"ERROR:CANNOT GET THE POST:\", data\n",
    "        continue\n",
    "    data = data['comments']\n",
    "    c2cs = data['data']\n",
    "    if 'next' in data['paging']:\n",
    "        while True:\n",
    "            url = data['paging']['next']\n",
    "            results = requests.get(url)\n",
    "            data = json.loads(results.text)\n",
    "            try:\n",
    "                c2cs += data['data']\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                print \"ERROR: time sleep 3 seconds\"\n",
    "                print data.keys()\n",
    "                c2cs  += data['data']\n",
    "            if 'next' not in data['paging']:\n",
    "                break\n",
    "    print 'PostID[%d] %s (c2cs:%d) time:%d'%(i, p['id'], len(c2cs), time.time()-start)\n",
    "    print p['permalink_url']\n",
    "    fname = directory + 'c2cment%d-%s.json'%(i, p['id'])\n",
    "    with open(fname, 'w') as fout:\n",
    "        json.dump(c2cs, fout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
